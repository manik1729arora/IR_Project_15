{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2 languages Hindi and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import pickle\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Hindi_Stopwords.txt\", encoding = \"UTF-8\")\n",
    "stopWords = file.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_e = open('Geeta_English.txt', encoding=\"UTF8\").read().split(\"\\n\")\n",
    "paras_h = open('gita_Hindi.txt', encoding=\"UTF8\").read().split(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and tokensing Hindi words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = {\n",
    "    1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],\n",
    "    2: [\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\", \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\"],\n",
    "    3: [\"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\"],\n",
    "    4: [\"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\"],\n",
    "    5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\"],\n",
    "}\n",
    "\n",
    "def stemming_hindi(text):\n",
    "    for L in 5, 4, 3, 2, 1:\n",
    "        if len(text) > L + 1:\n",
    "            for suf in suffixes[L]:\n",
    "                if text.endswith(suf):\n",
    "                    return text[:-L]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(string, stopWords):\n",
    "    punctuations=[\"।\",\";\",\",\",\":\",\"!\",'\"',\"?\",\":-\",\"-\",\"{\",\"(\",\"}\",\")\",\"_\",\"०\",\"S\",\"―\",\"=\",\"[\",\"]\",\"......\",\":-\",\".\",\"॥\",'”',\"|\",\"“\",\"'\"]\n",
    "    string = \"\".join([w if w not in punctuations else \" \" for w in string])  #To remove punctuations\n",
    "    tokens = string.split()\n",
    "    tokens = [stemming_hindi(word) for word in tokens if word not in stopWords]\n",
    "    tokens = [w for w in tokens if w not in stopWords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lis = {}\n",
    "for idx, para in enumerate(paras_h):\n",
    "    words = preprocess_text(para, stopWords)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in pos_lis.keys():\n",
    "            if idx in pos_lis[word].keys():\n",
    "                pos_lis[word][idx] += 1\n",
    "            else:\n",
    "                pos_lis[word][idx] = 1\n",
    "        else:\n",
    "            temp = {idx : 1}\n",
    "            pos_lis[word] = temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'wb') as file:\n",
    "    pickle.dump(pos_lis, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same way implementing for BM25 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(query, pos_lisl, l = 5, b = 0.75, k = 2):\n",
    "    q_tokens = preprocess_text(query,stopWords)\n",
    "    lengths = {}\n",
    "    N = len(paras_e)\n",
    "    avg_len = 0\n",
    "    for idx, para in enumerate(paras_e):\n",
    "        lengths[idx] = len(para)             #cal no of words of each file\n",
    "        avg_len += lengths[idx]\n",
    "    avg_len /= N\n",
    "    #Calculate idf of each token\n",
    "    idf = {}\n",
    "    for word in np.unique(q_tokens):\n",
    "        if word in pos_lisl.keys():\n",
    "            df = len(pos_lisl[word].keys())\n",
    "        else:\n",
    "            df = 0\n",
    "        idf[word] = np.log((N - df + 0.5) / (df + 0.5))\n",
    "    score = {}\n",
    "    for idx, para in enumerate(paras_e):\n",
    "        s = 0\n",
    "        for word in np.unique(q_tokens):\n",
    "            tf = 0\n",
    "            if word in pos_lisl.keys() and idx in pos_lisl[word].keys():\n",
    "                tf = pos_lisl[word][idx]\n",
    "            s += idf[word] * (tf * (k + 1)) / (k*(1 - b + b*lengths[idx]/avg_len) + tf)\n",
    "        score[idx] = s\n",
    "    return sorted(score, key = score.get, reverse=True)[:l]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching and Evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def get_translation(data, dest):\n",
    "    translator = Translator()\n",
    "    text = translator.translate(data, dest).text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "slokas = open(\"Shlokas.txt\",encoding=\"UTF-8\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, pos_lis, lang, l = 5):\n",
    "    query = get_translation(query, \"hi\")\n",
    "    tokens = preprocess_text(query, stopWords)\n",
    "    \n",
    "    query = \" \".join(tokens)\n",
    "\n",
    "    indxs = BM25(query, pos_lis, l)\n",
    "    print(indxs)\n",
    "    if lang == 'e':\n",
    "        return [f'{slokas[idx]}{paras_e[idx]}' for idx in indxs], indxs\n",
    "    else:\n",
    "        return [f'{slokas[idx]}{paras_h[idx]}' for idx in indxs], indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143, 167, 193, 100, 112]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['अर्जुन उवाच ।हे शत्रुओं का नाश करने वाले, जो पूजा के पात्र हैं, मैं अपने बाणों से युद्ध करूंगा।',\n",
       "  'सेनयोरुभयोर्मध्ये विषीदन्तमिदं वचः ॥ जो तुम्हारे लिए शोक नहीं करते उनके लिए तुम शोक नहीं करते और ज्ञान की बातें कहते हो',\n",
       "  'न जायते म्रियते वा कदाचिन्वह जो सदा के लिए वेदों को नष्ट कर देता है, यह अजन्मा और अविनाशी है।',\n",
       "  'यद्यप्येते न पश्यन्ति लोभोपहतचेतसः ।परिवार को नष्ट करना पाप है और मित्र को धोखा देना पाप है',\n",
       "  'ये दोष कुल का नाश करने वाले और जातिगत भ्रम पैदा करने वाले होते हैं।'],\n",
       " [143, 167, 193, 100, 112])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"That gives itself to follow shows of sense\"\n",
    "search(query,pos_lis,\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
