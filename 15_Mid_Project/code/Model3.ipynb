{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR THREE LAnguage as we discussed we are going to implement English,Hindi,gujrati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Amit\n",
      "[nltk_data]     Malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import pickle\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Hindi_Stopwords.txt\", encoding = \"UTF-8\")\n",
    "stopWords = file.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_e = open('Geeta_English.txt', encoding=\"UTF8\").read().split(\"\\n\")\n",
    "paras_h = open('gita_Hindi.txt', encoding=\"UTF8\").read().split(\"\\n\")\n",
    "paras_g = open('gita-gujrati.txt', encoding=\"UTF8\").read().split(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and tokensing Hindi words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = {\n",
    "    1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],\n",
    "    2: [\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\", \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\"],\n",
    "    3: [\"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\"],\n",
    "    4: [\"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\"],\n",
    "    5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\"],\n",
    "}\n",
    "\n",
    "def stemming_hindi(text):\n",
    "    for L in 5, 4, 3, 2, 1:\n",
    "        if len(text) > L + 1:\n",
    "            for suf in suffixes[L]:\n",
    "                if text.endswith(suf):\n",
    "                    return text[:-L]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(string, stopWords):\n",
    "    punctuations=[\"।\",\";\",\",\",\":\",\"!\",'\"',\"?\",\":-\",\"-\",\"{\",\"(\",\"}\",\")\",\"_\",\"०\",\"S\",\"―\",\"=\",\"[\",\"]\",\"......\",\":-\",\".\",\"॥\",'”',\"|\",\"“\",\"'\"]\n",
    "    string = \"\".join([w if w not in punctuations else \" \" for w in string])  #To remove punctuations\n",
    "    tokens = string.split()\n",
    "    tokens = [stemming_hindi(word) for word in tokens if word not in stopWords]\n",
    "    tokens = [w for w in tokens if w not in stopWords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lis = {}\n",
    "for idx, para in enumerate(paras_h):\n",
    "    words = preprocess_text(para, stopWords)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in pos_lis.keys():\n",
    "            if idx in pos_lis[word].keys():\n",
    "                pos_lis[word][idx] += 1\n",
    "            else:\n",
    "                pos_lis[word][idx] = 1\n",
    "        else:\n",
    "            temp = {idx : 1}\n",
    "            pos_lis[word] = temp.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(query, pos_lisl, l = 5, b = 0.75, k = 2):\n",
    "    q_tokens = preprocess_text(query,stopWords)\n",
    "    lengths = {}\n",
    "    N = len(paras_e)\n",
    "    avg_len = 0\n",
    "    for idx, para in enumerate(paras_e):\n",
    "        lengths[idx] = len(para)             #cal no of words of each file\n",
    "        avg_len += lengths[idx]\n",
    "    avg_len /= N\n",
    "    #Calculate idf of each token\n",
    "    idf = {}\n",
    "    for word in np.unique(q_tokens):\n",
    "        if word in pos_lisl.keys():\n",
    "            df = len(pos_lisl[word].keys())\n",
    "        else:\n",
    "            df = 0\n",
    "        idf[word] = np.log((N - df + 0.5) / (df + 0.5))\n",
    "    score = {}\n",
    "    for idx, para in enumerate(paras_e):\n",
    "        s = 0\n",
    "        for word in np.unique(q_tokens):\n",
    "            tf = 0\n",
    "            if word in pos_lisl.keys() and idx in pos_lisl[word].keys():\n",
    "                tf = pos_lisl[word][idx]\n",
    "            s += idf[word] * (tf * (k + 1)) / (k*(1 - b + b*lengths[idx]/avg_len) + tf)\n",
    "        score[idx] = s\n",
    "    return sorted(score, key = score.get, reverse=True)[:l]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching and Evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def get_translation(data, dest):\n",
    "    translator = Translator()\n",
    "    text = translator.translate(data, dest).text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "slokas = open(\"Shlokas.txt\",encoding=\"UTF-8\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, pos_lis, lang, l = 5):\n",
    "    query = get_translation(query, \"hi\")\n",
    "    tokens = preprocess_text(query, stopWords)\n",
    "    \n",
    "    query = \" \".join(tokens)\n",
    "\n",
    "    indxs = BM25(query, pos_lis, l)\n",
    "    print(indxs)\n",
    "    if lang == 'e':\n",
    "        return [f'{slokas[idx]}{paras_e[idx]}' for idx in indxs], indxs\n",
    "    elif lang== 'g':\n",
    "        return [f'{slokas[idx]}{paras_g[idx]}' for idx in indxs], indxs\n",
    "\n",
    "    else:\n",
    "        return [f'{slokas[idx]}{paras_h[idx]}' for idx in indxs], indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143, 167, 193, 100, 112]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['अर्जुन उवाच ।અર્જુને કહ્યું.',\n",
       "  'सेनयोरुभयोर्मध्ये विषीदन्तमिदं वचः ॥ નિરાશામાં ડૂબી ગયેલી બે સેનાઓ વચ્ચે આ શબ્દો સંભળાયા.',\n",
       "  'न जायते म्रियते वा कदाचिन्તે ક્યારેય જન્મતો નથી કે મૃત્યુ પામતો નથી',\n",
       "  'यद्यप्येते न पश्यन्ति लोभोपहतचेतसः ।જો કે આ લોકો જોતા નથી, તેમ છતાં તેમના મન લોભથી કાબુમાં હોય છે.',\n",
       "  ''],\n",
       " [143, 167, 193, 100, 112])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"That gives itself to follow shows of sense\"\n",
    "search(query,pos_lis,\"g\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "lengths = defaultdict(int)\n",
    "for idx, para in enumerate(paras_h, start=1):\n",
    "    lengths[idx] = len(para)\n",
    "\n",
    "idf = {}\n",
    "N = len(slokas)\n",
    "for word in pos_lis.keys():\n",
    "    df = len(pos_lis[word].keys())\n",
    "    idf[word] = np.log((N) / (df))\n",
    "\n",
    "# Calculate tf-idf score vector of each file\n",
    "norm_list = defaultdict(list)\n",
    "for idx, para in enumerate(paras_h, start=1):\n",
    "    for word in np.unique(para.split()):\n",
    "       tf = 0\n",
    "       if word in idf:\n",
    "            try:\n",
    "                if idx in pos_lis[word].keys():\n",
    "                    tf = pos_lis[word][idx] / lengths[idx]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            norm_list[idx].append(tf * idf[word])\n",
    "\n",
    "for idx in range(1, N):\n",
    "    norm_list[idx] = np.linalg.norm(norm_list[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cosine_sim(n_docs, query_vector, doc_vectors):\n",
    "    cosine_sims = []\n",
    "    for i in range(n_docs-1):\n",
    "        dot = np.dot(query_vector, doc_vectors[i])\n",
    "        query_norm = np.linalg.norm(query_vector)\n",
    "        doc_norm = norm_list[i]\n",
    "        cosine_sims.append(dot/((query_norm + 0.5)*(doc_norm + 0.5)))\n",
    "    return cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(query, main_dict):\n",
    "    n_slokas = len(slokas)\n",
    "    # finding document vectors\n",
    "    tfidf_dict = {} # dictionary which stores tfidf of each document\n",
    "    doc_vectors = [] \n",
    "    for i in range(n_slokas):\n",
    "        n_words = len(paras_h[i].split())\n",
    "        vector = []\n",
    "        for word in np.unique(query):\n",
    "            if word in main_dict.keys() and i in main_dict[word].keys():\n",
    "#                 tf = main_dict[word][i]/n_words\n",
    "                tf = main_dict[word][i]\n",
    "                idf = np.log((n_slokas+1)/(len(main_dict[word].keys())+1))\n",
    "                tf_idf = tf*idf\n",
    "                vector.append(tf_idf)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        doc_vectors.append(vector)\n",
    "    word_freq_query_dict = Counter(query)\n",
    "    # finding query vector\n",
    "    query_vector = []\n",
    "    # remember np.unique return result in alphabetical order\n",
    "    for word in np.unique(query):\n",
    "        if word in main_dict.keys():\n",
    "#             tf = word_freq_query_dict[word]/len(query)\n",
    "            tf = word_freq_query_dict[word]\n",
    "            idf = np.log((n_slokas+1)/(len(main_dict[word].keys())+1))\n",
    "            tf_idf = tf*idf\n",
    "            query_vector.append(tf_idf)\n",
    "        else:\n",
    "               query_vector.append(0)\n",
    "#     print(query_vector)\n",
    "    scores = find_cosine_sim(n_slokas, query_vector, doc_vectors)\n",
    "    return sorted(np.argsort(np.array(scores))[-5:], reverse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, pos_lis, lang, l = 5):\n",
    "    query = get_translation(query, \"hi\")\n",
    "    tokens = preprocess_text(query, stopWords)\n",
    "    \n",
    "    query = \" \".join(tokens)\n",
    "\n",
    "    indxs = BM25(query, pos_lis, l)\n",
    "    print(indxs)\n",
    "    if lang == 'e':\n",
    "        return [f'{slokas[idx]}{paras_e[idx]}' for idx in indxs], indxs\n",
    "    elif lang== 'g':\n",
    "        return [f'{slokas[idx]}{paras_g[idx]}' for idx in indxs], indxs\n",
    "\n",
    "    else:\n",
    "        return [f'{slokas[idx]}{paras_h[idx]}' for idx in indxs], indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143, 167, 193, 100, 112]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['अर्जुन उवाच ।',\n",
       "  'सेनयोरुभयोर्मध्ये विषीदन्तमिदं वचः ॥ Arjuna.',\n",
       "  \"न जायते म्रियते वा कदाचिन्While the Prince wept despairing 'twixt those hosts,\",\n",
       "  'यद्यप्येते न पश्यन्ति लोभोपहतचेतसः ।Triumph and domination, wealth and ease,',\n",
       "  ''],\n",
       " [143, 167, 193, 100, 112])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"That gives itself to follow shows of sense\"\n",
    "search(query,pos_lis,\"e\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION OF BM25 AND TF-IDF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\amit malik\\appdata\\roaming\\python\\python39\\site-packages (from rank_bm25) (1.20.2)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_models(documents, queries, relevant_docs, model_type):\n",
    "   \n",
    "    if model_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif model_type == 'bm25':\n",
    "        tokenized_docs = [doc.split() for doc in documents]\n",
    "        bm25 = BM25Okapi(tokenized_docs)\n",
    "    else:\n",
    "        raise ValueError('Model type must be \"bm25\" or \"tfidf\"')\n",
    "    \n",
    "    \n",
    "    if model_type == 'tfidf':\n",
    "        doc_vectors = vectorizer.fit_transform(documents)\n",
    "    elif model_type == 'bm25':\n",
    "        pass \n",
    "   \n",
    "    query_vectors = vectorizer.transform(queries)\n",
    "    similarities = cosine_similarity(query_vectors, doc_vectors)\n",
    "    num_queries = len(queries)\n",
    "    avg_precision = 0\n",
    "    avg_recall = 0\n",
    "    avg_f1 = 0\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        retrieved_docs = similarities[i].argsort()[::-1] \n",
    "        relevant = set(relevant_docs[i])\n",
    "        retrieved = set(retrieved_docs[:len(relevant_docs[i])])\n",
    "        true_positives = relevant.intersection(retrieved)\n",
    "        false_positives = retrieved - relevant\n",
    "        false_negatives = relevant - retrieved\n",
    "        \n",
    "        precision = len(true_positives) / (len(true_positives) + len(false_positives))\n",
    "        recall = len(true_positives) / (len(true_positives) + len(false_negatives))\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        avg_precision += precision\n",
    "        avg_recall += recall\n",
    "        avg_f1 += f1\n",
    "    \n",
    "    avg_precision /= num_queries\n",
    "    avg_recall /= num_queries\n",
    "    avg_f1 /= num_queries\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
