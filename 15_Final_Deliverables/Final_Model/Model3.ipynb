{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR SIX Language as we discussed we are going to implement English,Hindi,gujrati,punjabi,bengali,tamil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ujjwalgoel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_e = open('/Users/ujjwalgoel/Downloads/IR_Project_15-main/15_MidProject/Dataset/english.txt', encoding=\"UTF8\").read().split(\"\\n\")\n",
    "paras_h = open('/Users/ujjwalgoel/Downloads/IR_Project_15-main/15_MidProject/Dataset/hindi.txt', encoding=\"UTF8\").read().split(\"\\n\")\n",
    "paras_g = open('/Users/ujjwalgoel/Downloads/IR_Project_15-main/15_MidProject/Dataset/gujarati.txt', encoding=\"UTF8\").read().split(\"\\n\")\n",
    "paras_p = open('/Users/ujjwalgoel/Downloads/IR_Project_15-main/15_MidProject/Dataset/punjabi.txt', encoding=\"UTF8\").read().split(\"\\n\")\n",
    "paras_t = open('/Users/ujjwalgoel/Downloads/IR_Project_15-main/15_MidProject/Dataset/tamil.txt', encoding=\"UTF8\").read().split(\"\\n\")\n",
    "paras_b = open('/Users/ujjwalgoel/Downloads/IR_Project_15-main/15_MidProject/Dataset/bengali.txt', encoding=\"UTF8\").read().split(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and tokensing Hindi words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = {\n",
    "    1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],\n",
    "    2: [\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\", \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\"],\n",
    "    3: [\"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\"],\n",
    "    4: [\"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\"],\n",
    "    5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\"],\n",
    "}\n",
    "\n",
    "def stemming_hindi(text):\n",
    "    for L in 5, 4, 3, 2, 1:\n",
    "        if len(text) > L + 1:\n",
    "            for suf in suffixes[L]:\n",
    "                if text.endswith(suf):\n",
    "                    return text[:-L]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as st\n",
    "\n",
    "def preprocess_text(string, stopWords):\n",
    "    # Define a set of punctuation marks to remove\n",
    "    punctuations = set(st.punctuation) | set([\"०\", \"S\", \"―\", \"=\", \"॥\", \"”\", \"|\", \"“\", \"'\", \"।\", \";\", \",\", \":\", \"!\", '\"', \"?\", \"[\", \"]\", \"......\", \":-\", \".\", \":-\", \"{\", \"(\", \"}\", \")\"])\n",
    "    \n",
    "    # Remove punctuations and split the string into tokens\n",
    "    tokens = string.translate(str.maketrans(\"\", \"\", \"\".join(punctuations))).split()\n",
    "    \n",
    "    # Apply stemming and remove stop words from the tokens\n",
    "    tokens = [stemming_hindi(word) for word in tokens if word not in stopWords]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lis = {}\n",
    "for idx, para in enumerate(paras_h):\n",
    "    words = preprocess_text(para, stopWords)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in pos_lis.keys():\n",
    "            if idx in pos_lis[word].keys():\n",
    "                pos_lis[word][idx] += 1\n",
    "            else:\n",
    "                pos_lis[word][idx] = 1\n",
    "        else:\n",
    "            temp = {idx : 1}\n",
    "            pos_lis[word] = temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_lis))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def bm25(query, paragraphs, pos_lisl, stop_words, l, b=0.75, k=2):\n",
    "    q_tokens = preprocess_text(query, stop_words)\n",
    "    lengths = [len(p.split()) for p in paragraphs]\n",
    "    N = len(paragraphs)\n",
    "    avg_len = sum(lengths) / N\n",
    "\n",
    "    idf = {}\n",
    "    for word in np.unique(q_tokens):\n",
    "        if word in pos_lisl:\n",
    "            df = len(pos_lisl[word])\n",
    "        else:\n",
    "            df = 0\n",
    "        idf[word] = np.log((N - df + 0.5) / (df + 0.5))\n",
    "\n",
    "    score = defaultdict(float)\n",
    "    for idx, para in enumerate(paragraphs):\n",
    "        para_tokens = para.split()\n",
    "        for word in q_tokens:\n",
    "            tf = para_tokens.count(word)\n",
    "            if word in pos_lisl and idx in pos_lisl[word]:\n",
    "                tf = pos_lisl[word][idx]\n",
    "            score[idx] += idf[word] * (tf * (k + 1)) / (k * (1 - b + b * lengths[idx] / avg_len) + tf)\n",
    "\n",
    "    return sorted(score, key=score.get, reverse=True)[:l]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, pos_lis, lang, l):\n",
    "    # print(query)\n",
    "    # print(pos_lis)\n",
    "    query = get_translation(query, \"hi\")\n",
    "    # print(query)\n",
    "    # print(query)\n",
    "    tokens = preprocess_text(query, stopWords)\n",
    "   \n",
    "    query = \" \".join(tokens)\n",
    "\n",
    "    indxs = bm25(query, paras_e,pos_lis,stopWords, l,0.75,2)\n",
    "    # print(indxs)\n",
    "    if lang == 'e':\n",
    "        return [f'{paras_e[idx]}' for idx in indxs], indxs\n",
    "    elif lang== 'g':\n",
    "        return [f'{paras_g[idx]}' for idx in indxs], indxs\n",
    "\n",
    "    elif lang=='h':\n",
    "        return [f'{paras_h[idx]}' for idx in indxs], indxs\n",
    "    elif lang=='b':\n",
    "        return [f'{paras_b[idx]}' for idx in indxs], indxs\n",
    "    elif lang=='t':\n",
    "        return [f'{paras_t[idx]}' for idx in indxs], indxs\n",
    "    elif lang=='p':\n",
    "        return [f'{paras_p[idx]}' for idx in indxs], indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def get_translation(data, dest):\n",
    "    translator = Translator()\n",
    "    text = translator.translate(data, dest).text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shloka(shloka):\n",
    "    n =len(shloka.split())\n",
    "    shloka = shloka.split()\n",
    "    idx1 = n//4\n",
    "    idx2 = n//2\n",
    "    idx3 = (3*n)//4\n",
    "\n",
    "    print(\" \".join(shloka[:idx1]))\n",
    "    print(\" \".join(shloka[idx1:idx2]))\n",
    "    print(\" \".join(shloka[idx2:idx3]))\n",
    "    print(\" \".join(shloka[idx3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 relevant documents for the query \"  praise shame evil lust  \" in the asked language are : \n",
      "\n",
      "shloka  1  (index =  355  ) =>  \n",
      "\n",
      "ਜੋ ਲੋਕ ਮੈਨੂੰ ਅਜੰਮੇ, ਅਨਾਦਿ ਅਤੇ ਸਾਰੇ\n",
      "ਬ੍ਰਹਿਮੰਡਾਂ ਦੇ ਸੁਆਮੀ ਦੇ ਰੂਪ ਵਿੱਚ ਜਾਣਦੇ ਹਨ,\n",
      "ਕੇਵਲ ਉਹ ਮਨੁੱਖਾਂ ਵਿੱਚ ਭੁਲੇਖੇ ਤੋਂ ਮੁਕਤ\n",
      "ਅਤੇ ਸਾਰੀਆਂ ਬੁਰਾਈਆਂ ਤੋਂ ਮੁਕਤ ਹੋ ਜਾਂਦੇ ਹਨ।\n",
      "\n",
      "shloka  2  (index =  362  ) =>  \n",
      "\n",
      "ਉਨ੍ਹਾਂ ਦੀ ਦਇਆ ਦੇ ਕਾਰਨ, ਮੈਂ ਉਨ੍ਹਾਂ\n",
      "ਦੇ ਹਿਰਦੇ ਵਿੱਚ ਵਾਸ ਕਰਦਾ ਹਾਂ ਅਤੇ\n",
      "ਗਿਆਨ ਦੇ ਪ੍ਰਕਾਸ਼ ਦੀਵੇ ਨਾਲ ਅੰਧਕਾਰ ਤੋਂ\n",
      "ਪੈਦਾ ਹੋਈ ਅਗਿਆਨਤਾ ਦਾ ਨਾਸ ਕਰਦਾ ਹਾਂ।\n",
      "\n",
      "shloka  3  (index =  628  ) =>  \n",
      "\n",
      "ਕਿਸੇ ਨੂੰ ਆਪਣੇ ਸੁਭਾਅ ਤੋਂ ਪੈਦਾ ਹੋਏ ਕਰਤੱਵਾਂ ਨੂੰ ਨਹੀਂ ਛੱਡਣਾ\n",
      "ਚਾਹੀਦਾ, ਭਾਵੇਂ ਕੋਈ ਉਨ੍ਹਾਂ ਵਿੱਚ ਨੁਕਸ ਦੇਖਦਾ ਹੋਵੇ। ਹੇ ਕੁੰਤੀ ਦੇ\n",
      "ਪੁੱਤਰ! ਅਸਲ ਵਿੱਚ ਹਰ ਕਿਸਮ ਦੇ ਉਦਯੋਗ ਕਿਸੇ ਨਾ ਕਿਸੇ ਬੁਰਾਈ\n",
      "ਨਾਲ ਢੱਕੇ ਹੋਏ ਹਨ ਜਿਵੇਂ ਅੱਗ ਧੂੰਏਂ ਨਾਲ ਢੱਕੀ ਹੋਈ ਹੈ।\n",
      "\n",
      "shloka  4  (index =  544  ) =>  \n",
      "\n",
      "ਸੈਕੜੇ ਇੱਛਾਵਾਂ ਦੇ ਬੰਧਨ ਵਿੱਚ ਰਹਿ ਕੇ, ਕਾਮ\n",
      "ਕ੍ਰੋਧ ਤੋਂ ਪ੍ਰੇਰਿਤ ਹੋ ਕੇ, ਉਹ ਨਜਾਇਜ਼ ਧਨ\n",
      "ਇਕੱਠਾ ਕਰਨ ਵਿੱਚ ਲੱਗੇ ਰਹਿੰਦੇ ਹਨ। ਉਹ ਇਹ\n",
      "ਸਭ ਕੁਝ ਇੰਦਰੀਆਂ ਦੀ ਤ੍ਰਿਪਤੀ ਲਈ ਕਰਦੇ ਹਨ।\n",
      "\n",
      "shloka  5  (index =  142  ) =>  \n",
      "\n",
      "ਪਰਮ ਆਤਮਾ ਸ਼੍ਰੀ ਕ੍ਰਿਸ਼ਨ ਜੀ ਕਹਿੰਦੇ ਹਨ - ਕੇਵਲ ਵਾਸਨਾ\n",
      "ਜੋ ਰਜੋਗੁਣ ਦੇ ਸੰਪਰਕ ਵਿੱਚ ਆਉਣ ਨਾਲ ਪੈਦਾ ਹੁੰਦੀ ਹੈ ਅਤੇ\n",
      "ਬਾਅਦ ਵਿੱਚ ਕ੍ਰੋਧ ਦਾ ਰੂਪ ਧਾਰ ਲੈਂਦੀ ਹੈ, ਉਸਨੂੰ ਪਾਪ\n",
      "ਦੇ ਰੂਪ ਵਿੱਚ ਸੰਸਾਰ ਦਾ ਸਭ ਭਸਮ ਕਰਨ ਵਾਲਾ ਦੁਸ਼ਮਣ ਸਮਝੋ।\n"
     ]
    }
   ],
   "source": [
    "query =\"praise shame evil lust\"\n",
    "result = search(query,pos_lis,\"p\",5)\n",
    "\n",
    "print(\"The top 5 relevant documents for the query \\\" \",query,\" \\\" in the asked language are : \")\n",
    "for i in range(5):\n",
    "    print(\"\\nshloka \",i+1,\" (index = \",result[1][i],\" ) =>  \\n\")\n",
    "\n",
    "    print_shloka(result[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 relevant documents for the query \"  Actions based upon sacrifice and charity  \" in the asked language are : \n",
      "\n",
      "shloka  1  (index =  586  ) =>  \n",
      "\n",
      "યજ્ઞ, દાન અને તપ પર આધારિત\n",
      "કર્મોનો ક્યારેય ત્યાગ ન કરવો જોઈએ. તેઓ\n",
      "ચોક્કસપણે થવું જોઈએ. યજ્ઞ, દાન અને તપ\n",
      "ખરેખર મહાન આત્માઓને પણ શુદ્ધ કરે છે.\n",
      "\n",
      "shloka  2  (index =  584  ) =>  \n",
      "\n",
      "કેટલાક ઋષિમુનિઓ જાહેર કરે છે કે તમામ પ્રકારના\n",
      "કાર્યોને દોષપૂર્ણ માનીને છોડી દેવા જોઈએ, પરંતુ અન્ય\n",
      "વિદ્વાનો ભારપૂર્વક કહે છે કે યજ્ઞ, દાન અને\n",
      "તપસ્યા જેવા કાર્યોને ક્યારેય છોડી દેવા જોઈએ નહીં.\n",
      "\n",
      "shloka  3  (index =  566  ) =>  \n",
      "\n",
      "શ્રદ્ધા વિના અને શાસ્ત્રોની આજ્ઞાથી વિપરિત યજ્ઞ,\n",
      "જેમાં અન્ન અર્પણ કરવામાં આવતું નથી, મંત્રોનો\n",
      "જાપ કરવામાં આવતો નથી અને દાન આપવામાં\n",
      "આવતું નથી, આવો યજ્ઞ સ્વભાવે તમોગુણી છે.\n",
      "\n",
      "shloka  4  (index =  575  ) =>  \n",
      "\n",
      "આવા દાન જે અપવિત્ર સ્થાને અને\n",
      "અયોગ્ય સમયે અયોગ્ય વ્યક્તિઓને અથવા આદર વિના\n",
      "અથવા તિરસ્કાર સાથે આપવામાં આવે છે,\n",
      "તે તમોગુણી સ્વભાવનું દાન માનવામાં આવે છે.\n",
      "\n",
      "shloka  5  (index =  577  ) =>  \n",
      "\n",
      "તેથી જ વેદના ઘાતાક દ્વારા\n",
      "યજ્ઞ, દાન અને તપસ્યા જેવા કાર્યો\n",
      "વેદના નિયમોની સૂચનાઓ અનુસાર 'ઓમ'\n",
      "ઉચ્ચાર કરીને શરૂ કરવામાં આવે છે.\n"
     ]
    }
   ],
   "source": [
    "query =\"Actions based upon sacrifice and charity\"\n",
    "result = search(query,pos_lis,\"g\",5)\n",
    "\n",
    "print(\"The top 5 relevant documents for the query \\\" \",query,\" \\\" in the asked language are : \")\n",
    "for i in range(5):\n",
    "    print(\"\\nshloka \",i+1,\" (index = \",result[1][i],\" ) =>  \\n\")\n",
    "\n",
    "    print_shloka(result[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 relevant documents for the query \"  Actions based upon sacrifice and charity  \" in the asked language are : \n",
      "\n",
      "shloka  1  (index =  586  ) =>  \n",
      "\n",
      "Actions based upon sacrifice, charity, and penance\n",
      "should never be abandoned; they must certainly be\n",
      "performed. Indeed, acts of sacrifice, charity, and penance\n",
      "are purifying even for those who are wise\n",
      "\n",
      "shloka  2  (index =  584  ) =>  \n",
      "\n",
      "Some learned people declare that all kinds\n",
      "of actions should be given up as\n",
      "evil, while others maintain that acts of\n",
      "sacrifice, charity, and penance should never be abandoned\n",
      "\n",
      "shloka  3  (index =  566  ) =>  \n",
      "\n",
      "Sacrifice devoid of faith and contrary to the\n",
      "injunctions of the scriptures, in which no food is\n",
      "offered, no mantras chanted, and no donation made,\n",
      "is to be considered in the mode of ignorance\n",
      "\n",
      "shloka  4  (index =  575  ) =>  \n",
      "\n",
      "And that charity, which is given at\n",
      "the wrong place and wrong time to unworthy\n",
      "persons, without showing respect, or with contempt, is\n",
      "held to be of the nature of nescience\n",
      "\n",
      "shloka  5  (index =  577  ) =>  \n",
      "\n",
      "Therefore, when performing acts of sacrifice,\n",
      "offering charity, or undertaking penance, expounders of\n",
      "the Vedas always begin by uttering “Om”\n",
      "according to the prescriptions of Vedic injunctions\n"
     ]
    }
   ],
   "source": [
    "query =\"Actions based upon sacrifice and charity\"\n",
    "result = search(query,pos_lis,\"e\",5)\n",
    "\n",
    "print(\"The top 5 relevant documents for the query \\\" \",query,\" \\\" in the asked language are : \")\n",
    "for i in range(5):\n",
    "    print(\"\\nshloka \",i+1,\" (index = \",result[1][i],\" ) =>  \\n\")\n",
    "\n",
    "    print_shloka(result[0][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def calculate_lengths(paras_h):\n",
    "    lengths = defaultdict(int)\n",
    "    for idx, para in enumerate(paras_h, start=1):\n",
    "        lengths[idx] = len(para.split())\n",
    "    return lengths\n",
    "\n",
    "def calculate_idf(pos_lis, N):\n",
    "    idf = {}\n",
    "    for word in pos_lis.keys():\n",
    "        df = len(pos_lis[word].keys())\n",
    "        idf[word] = np.log((N) / (df))\n",
    "    return idf\n",
    "\n",
    "def calculate_norm_list(paras_h, idf, pos_lis, lengths):\n",
    "    norm_list = defaultdict(list)\n",
    "    for idx, para in enumerate(paras_h, start=1):\n",
    "        for word in np.unique(para.split()):\n",
    "            tf = 0\n",
    "            if word in idf:\n",
    "                try:\n",
    "                    if idx in pos_lis[word].keys():\n",
    "                        tf = pos_lis[word][idx] / lengths[idx]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                norm_list[idx].append(tf * idf[word])\n",
    "    return norm_list\n",
    "\n",
    "def calculate_norms(N, norm_list):\n",
    "    for idx in range(1, N+1):\n",
    "        norm_list[idx] = np.linalg.norm(norm_list[idx])\n",
    "    return norm_list\n",
    "\n",
    "def calculate_tfidf(paras_h, pos_lis):\n",
    "    lengths = calculate_lengths(paras_h)\n",
    "    N = len(paras_h)\n",
    "    idf = calculate_idf(pos_lis, N)\n",
    "    norm_list = calculate_norm_list(paras_h, idf, pos_lis, lengths)\n",
    "    norm_list = calculate_norms(N, norm_list)\n",
    "    return norm_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_scores = calculate_tfidf(paras_h, pos_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cosine_sim(n_docs, query_vector, doc_vectors):\n",
    "    cosine_sims = []\n",
    "    for i in range(n_docs-1):\n",
    "        dot = np.dot(query_vector, doc_vectors[i])\n",
    "        query_norm = np.linalg.norm(query_vector)\n",
    "        doc_norm = norm_list[i]\n",
    "        cosine_sims.append(dot/((query_norm + 0.5)*(doc_norm + 0.5)))\n",
    "    return cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(query, main_dict,l):\n",
    "    n_slokas = len(paras_h)\n",
    "    tfidf_dict = {} # dictionary which stores tfidf of each document\n",
    "    doc_vectors = [] \n",
    "    for i in range(n_slokas):\n",
    "        vector = []\n",
    "        for word in set(query):\n",
    "            if word in main_dict and i in main_dict[word]:\n",
    "                tf = main_dict[word][i]\n",
    "                idf = np.log((n_slokas+1)/(len(main_dict[word])+1))\n",
    "                tf_idf = tf*idf\n",
    "                vector.append(tf_idf)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        doc_vectors.append(vector)\n",
    "    query_vector = []\n",
    "    for word in set(query):\n",
    "        if word in main_dict:\n",
    "            tf = query.count(word)\n",
    "            idf = np.log((n_slokas+1)/(len(main_dict[word])+1))\n",
    "            tf_idf = tf*idf\n",
    "            query_vector.append(tf_idf)\n",
    "        else:\n",
    "            query_vector.append(0)\n",
    "    scores = find_cosine_sim(n_slokas, query_vector, doc_vectors)\n",
    "    return sorted(np.argsort(np.array(scores))[:l], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, pos_lis, lang, l):\n",
    "    query = get_translation(query, \"hi\")\n",
    "    tokens = preprocess_text(query, stopWords)\n",
    "    \n",
    "    query = \" \".join(tokens)\n",
    "\n",
    "    indxs = bm25(query, pos_lis, l)\n",
    "    # print(indxs)\n",
    "    if lang == 'e':\n",
    "        return [f'{paras_e[idx]}' for idx in indxs], indxs\n",
    "    elif lang== 'g':\n",
    "        return [f'{paras_g[idx]}' for idx in indxs], indxs\n",
    "\n",
    "    elif lang=='h':\n",
    "        return [f'{paras_h[idx]}' for idx in indxs], indxs\n",
    "    elif lang=='b':\n",
    "        return [f'{paras_b[idx]}' for idx in indxs], indxs\n",
    "    elif lang=='t':\n",
    "        return [f'{paras_t[idx]}' for idx in indxs], indxs\n",
    "    elif lang=='p':\n",
    "        return [f'{paras_p[idx]}' for idx in indxs], indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries =[]\n",
    "relevant = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 relevant documents using TF_IDF model for the query \"  જબરદસ્ત અવાજ આખા આકાશ અને પૃથ્વી પર ગર્જ્યો  \" in the asked language are : \n",
      "\n",
      "shloka  1  (index =  14  ) =>  \n",
      "\n",
      "The terrific sound thundered\n",
      "across the sky and the\n",
      "earth, and shattered the hearts\n",
      "of your sons, O Dhritarasthra\n",
      "\n",
      "shloka  2  (index =  268  ) =>  \n",
      "\n",
      "Earth, water, fire, air,\n",
      "space, mind, intellect, and\n",
      "ego—these are eight components\n",
      "of My material energy\n",
      "\n",
      "shloka  3  (index =  403  ) =>  \n",
      "\n",
      "If a thousand suns were\n",
      "to blaze forth together in the\n",
      "sky, they would not match\n",
      "the splendor of that great form\n",
      "\n",
      "shloka  4  (index =  411  ) =>  \n",
      "\n",
      "The space between heaven and earth and all\n",
      "the directions is pervaded by You alone. Seeing Your\n",
      "wondrous and terrible form, I see the three\n",
      "worlds trembling in fear, O Greatest of all beings\n",
      "\n",
      "shloka  5  (index =  72  ) =>  \n",
      "\n",
      "If you fight, you will either be slain on the\n",
      "battlefield and go to the celestial abodes, or you will\n",
      "gain victory and enjoy the kingdom on earth. Therefore arise\n",
      "with determination, O son of Kunti, and be prepared to fight\n"
     ]
    }
   ],
   "source": [
    "query =\"જબરદસ્ત અવાજ આખા આકાશ અને પૃથ્વી પર ગર્જ્યો\"\n",
    "result = search(query,pos_lis,\"e\",l=5)\n",
    "queries.append(query)\n",
    "relevant.append(result[1])\n",
    "print(\"The top 5 relevant documents using TF_IDF model for the query \\\" \",query,\" \\\" in the asked language are : \")\n",
    "for i in range(5):\n",
    "    print(\"\\nshloka \",i+1,\" (index = \",result[1][i],\" ) =>  \\n\")\n",
    "\n",
    "    print_shloka(result[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 relevant documents using TF_IDF model for the query \"  என் உடம்பெல்லாம் நடுங்குகிறது; என் தலைமுடி உதிர்ந்து நிற்கிறது  \" in the asked language are : \n",
      "\n",
      "shloka  1  (index =  23  ) =>  \n",
      "\n",
      "मेरा सारा शरीर काँप रहा है, मेरे शरीर के रोएं खड़े हो रहे हैं, मेरा धनुष ‘गाण्डीव' मेरे हाथ से सरक रहा है और\n",
      "मेरी पूरी त्वचा में जलन हो रही है। मेरा मन उलझ रहा है और मुझे घबराहट हो रही है। अब मैं यहाँ और अधिक\n",
      "खड़ा रहने में समर्थ नहीं हूँ। केशी राक्षस को मारने वाले हे केशव! मुझे केवल अमंगल के लक्षण दिखाई दे रहे हैं। युद्ध में\n",
      "अपने वंश के बंधु बान्धवों का वध करने में मुझे कोई अच्छाई नही दिखाई देती है और उन्हें मारकर मैं कैसे सुख पा सकता हूँ?\n",
      "\n",
      "shloka  2  (index =  489  ) =>  \n",
      "\n",
      "अंतरिक्ष सबको अपने में धारण कर लेता है लेकिन सूक्ष्म होने के\n",
      "कारण जिसे यह धारण किए रहता है उसमें लिप्त नहीं होता। इसी\n",
      "प्रकार से यद्यपि आत्मा चेतना के रूप में पूरे शरीर में व्याप्त\n",
      "रहती है फिर भी आत्मा शरीर के धर्म से प्रभावित नहीं होती।\n",
      "\n",
      "shloka  3  (index =  339  ) =>  \n",
      "\n",
      "किन्तु जो लोग सदैव मेरे बारे में सोचते हैं और मेरी\n",
      "अनन्य भक्ति में लीन रहते हैं एवं जिनका मन सदैव मुझमें तल्लीन\n",
      "रहता है, उनकी जो आवश्यकताएँ होती हैं उन्हें मैं पूरा करता हूँ\n",
      "और जो कुछ उनके स्वामित्व में होता है, उसकी रक्षा करता हूँ।\n",
      "\n",
      "shloka  4  (index =  52  ) =>  \n",
      "\n",
      "जो पूरे शरीर में व्याप्त\n",
      "है, उसे ही तुम अविनाशी समझो।\n",
      "उस अनश्वर आत्मा को नष्ट करने\n",
      "मे कोई भी समर्थ नहीं है।\n",
      "\n",
      "shloka  5  (index =  490  ) =>  \n",
      "\n",
      "जिस प्रकार से एक सूर्य समस्त\n",
      "ब्रह्माण्ड को प्रकाशित करता है उसी\n",
      "प्रकार से आत्मा चेतना शक्ति के\n",
      "साथ पूरे शरीर को प्रकाशित करती है।\n"
     ]
    }
   ],
   "source": [
    "query =\"என் உடம்பெல்லாம் நடுங்குகிறது; என் தலைமுடி உதிர்ந்து நிற்கிறது\"\n",
    "result = search(query,pos_lis,\"h\",l=5)\n",
    "queries.append(query)\n",
    "relevant.append(result[1])\n",
    "print(\"The top 5 relevant documents using TF_IDF model for the query \\\" \",query,\" \\\" in the asked language are : \")\n",
    "for i in range(5):\n",
    "    print(\"\\nshloka \",i+1,\" (index = \",result[1][i],\" ) =>  \\n\")\n",
    "\n",
    "    print_shloka(result[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 relevant documents for the query \"  qualities and activities  \" in the asked language are : \n",
      "\n",
      "shloka  1  (index =  537  ) =>  \n",
      "\n",
      "The divine qualities lead to liberation, while\n",
      "the demoniac qualities are the cause for a\n",
      "continuing destiny of bondage. Grieve not, O\n",
      "Arjun, as you were born with saintly virtues\n",
      "\n",
      "shloka  2  (index =  293  ) =>  \n",
      "\n",
      "Those who take shelter in Me, striving\n",
      "for liberation from old-age and death, come\n",
      "to know the Brahman, the individual self,\n",
      "and the entire field of karmic action\n",
      "\n",
      "shloka  3  (index =  31  ) =>  \n",
      "\n",
      "Through the evil deeds of those who\n",
      "destroy the family tradition and thus give\n",
      "rise to unwanted progeny, a variety of\n",
      "social and family welfare activities are ruined\n",
      "\n",
      "shloka  4  (index =  509  ) =>  \n",
      "\n",
      "Arjun inquired: What are the characteristics of\n",
      "those who have gone beyond the three guṇas,\n",
      "O Lord? How do they act? How do\n",
      "they go beyond the bondage of the guṇas\n",
      "\n",
      "shloka  5  (index =  621  ) =>  \n",
      "\n",
      "The duties of the Brahmins,\n",
      "Kshatriyas, Vaishyas, and Shudras—are distributed according\n",
      "to their qualities, in accordance with\n",
      "their guṇas (and not by birth)\n",
      "\n",
      "shloka  6  (index =  507  ) =>  \n",
      "\n",
      "When wise persons see that in all work\n",
      "there is no agent of action other than the\n",
      "three guṇas, and they know Me to be\n",
      "transcendental to these guṇas, they attain My divine nature\n",
      "\n",
      "shloka  7  (index =  496  ) =>  \n",
      "\n",
      "O mighty-armed Arjun, the material energy\n",
      "consists of three guṇas (modes)—sattva (goodness), rajas\n",
      "(passion), and tamas (ignorance). These modes bind\n",
      "the eternal soul to the perishable body\n",
      "\n",
      "shloka  8  (index =  538  ) =>  \n",
      "\n",
      "There are two kinds of beings in this world—those\n",
      "endowed with a divine nature and those possessing a demoniac\n",
      "nature. I have described the divine qualities in detail,\n",
      "O Arjun. Now hear from me about the demoniac nature\n",
      "\n",
      "shloka  9  (index =  278  ) =>  \n",
      "\n",
      "My divine energy Maya, consisting of\n",
      "the three modes of nature, is very\n",
      "difficult to overcome. But those who\n",
      "surrender unto Me cross over it easily\n",
      "\n",
      "shloka  10  (index =  609  ) =>  \n",
      "\n",
      "Hear now, O Arjun, of the\n",
      "distinctions of intellect and determination, according\n",
      "to the three modes of material\n",
      "nature, as I describe them in detail\n"
     ]
    }
   ],
   "source": [
    "query =\"qualities and activities\"\n",
    "result = search(query,pos_lis,\"e\",l=10)\n",
    "queries.append(query)\n",
    "relevant.append(result[1])\n",
    "print(\"The top 20 relevant documents for the query \\\" \",query,\" \\\" in the asked language are : \")\n",
    "for i in range(10):\n",
    "    print(\"\\nshloka \",i+1,\" (index = \",result[1][i],\" ) =>  \\n\")\n",
    "\n",
    "    print_shloka(result[0][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION OF BM25 AND TF-IDF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(documents, queries, relevant_docs, model_type):\n",
    "    if model_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        doc_vectors = vectorizer.fit_transform(documents)\n",
    "        query_vectors = vectorizer.transform(queries)\n",
    "        similarities = cosine_similarity(query_vectors, doc_vectors)\n",
    "    elif model_type == 'bm25':\n",
    "        tokenized_docs = [doc.split() for doc in documents]\n",
    "        bm25 = BM25Okapi(tokenized_docs)\n",
    "        similarities = [bm25.get_scores(query.split()) for query in queries]\n",
    "    else:\n",
    "        raise ValueError('Model type must be \"bm25\" or \"tfidf\"')\n",
    "    \n",
    "    num_queries = len(queries)\n",
    "    avg_precision = 0\n",
    "    avg_recall = 0\n",
    "    avg_f1 = 0\n",
    "    mean_average_precision = 0\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        retrieved_docs = sorted(range(len(similarities[i])), key=lambda k: similarities[i][k], reverse=True)\n",
    "        # print(retrieved_docs)\n",
    "     \n",
    "        relevant = set(relevant_docs[i])\n",
    "        retrieved = set(retrieved_docs[:len(relevant_docs[i])])\n",
    "        true_positives = relevant.intersection(retrieved)\n",
    "        # print(true_positives)\n",
    "        false_positives = retrieved - relevant \n",
    "        false_negatives = relevant - retrieved \n",
    "        # print(false_positives)\n",
    "        # print(false_negatives)\n",
    "        len_true_positives = len(true_positives)+1\n",
    "        len_false_positives = len(false_positives)+1\n",
    "        len_false_negatives = len(false_negatives)+1\n",
    "        precision = len_true_positives / (len_true_positives + len_false_positives)\n",
    "        recall = len_true_positives / (len_true_positives + len_false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        avg_precision += precision\n",
    "        avg_recall += recall\n",
    "        avg_f1 += f1\n",
    "\n",
    "        ap = 0\n",
    "        num_relevant = len(relevant)\n",
    "        for j in range(num_relevant):\n",
    "            if retrieved_docs[j] in relevant:\n",
    "                ap += len(relevant.intersection(set(retrieved_docs[:j+1]))) / (j+1)\n",
    "        ap /= num_relevant\n",
    "\n",
    "        mean_average_precision += ap\n",
    "    \n",
    "    avg_precision /= num_queries\n",
    "    avg_recall /= num_queries\n",
    "    avg_f1 /= num_queries\n",
    "    mean_average_precision /= num_queries\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1,mean_average_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = evaluate_models(paras_e,queries,relevant,\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.336038961038961\n",
      "recall =  0.336038961038961\n",
      "f1 =  0.336038961038961\n",
      "map =  0.22477580404631492\n"
     ]
    }
   ],
   "source": [
    "print(\"precision = \", a)\n",
    "print(\"recall = \", b)\n",
    "print(\"f1 = \", c)\n",
    "print(\"map = \", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = evaluate_models(paras_e,queries,relevant,\"bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.3147546897546898\n",
      "recall =  0.3147546897546898\n",
      "f1 =  0.3147546897546898\n",
      "map =  0.1991463793988949\n"
     ]
    }
   ],
   "source": [
    "print(\"precision = \", a)\n",
    "print(\"recall = \", b)\n",
    "print(\"f1 = \", c)\n",
    "print(\"map = \", d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
